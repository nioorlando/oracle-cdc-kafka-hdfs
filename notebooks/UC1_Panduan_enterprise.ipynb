{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "16c049bb",
   "metadata": {},
   "source": [
    "\n",
    "# ðŸ“˜ Enterprise Use Case â€” Day 1  \n",
    "**Oracle â†’ Debezium â†’ Kafka â†’ HDFS**\n",
    "\n",
    "Panduan ini berisi langkah-langkah praktikum **Enterprise CDC pipeline**.  \n",
    "Semua perintah dijalankan di **Terminal**, bukan langsung di notebook.  \n",
    "Notebook ini hanya sebagai **panduan dokumentasi**.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”· Topology\n",
    "Berikut adalah arsitektur alur data yang akan dibangun:\n",
    "\n",
    "![](img/ora_debezium_hdfs.svg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0110cd2e",
   "metadata": {},
   "source": [
    "\n",
    "## 1) Prerequisites & Environment\n",
    "\n",
    "- **Cluster**: CDP/HDP production-like cluster dengan Kerberos & TLS.  \n",
    "- **Access**: Pastikan sudah mendapat Kerberos ticket (`kinit`).  \n",
    "- **Tools**: Kafka CLI, HDFS CLI, Spark 3, dan akses SMM.  \n",
    "- **Security**: JAAS & truststore sudah tersedia di node.\n",
    "\n",
    "ðŸ“Œ Semua variabel seperti `<username>` ganti sesuai user trainee masing-masing.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f46ee56",
   "metadata": {},
   "source": [
    "\n",
    "## 2) Client Security Setup\n",
    "\n",
    "Buka Terminal, jalankan perintah ini untuk set JAAS config:\n",
    "\n",
    "```bash\n",
    "export KAFKA_OPTS=\"-Djava.security.auth.login.config=/etc/kafka/kafka_client_jaas.conf\"\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48cb18bf",
   "metadata": {},
   "source": [
    "\n",
    "## 3) Create Topic (Enterprise Standard)\n",
    "\n",
    "Gunakan **naming convention**: `<env>.<domain>.<usecase>.demo.<username>`\n",
    "\n",
    "Contoh perintah:\n",
    "\n",
    "```bash\n",
    "kafka-topics --bootstrap-server djp-training-sbox.dla-dataplatform.internal:9093   --command-config /tmp/producer_non_jaas.properties   --create --if-not-exists   --topic dev.djp.test-topic.<username>   --partitions 3   --replication-factor 1\n",
    "```\n",
    "\n",
    "Cek daftar topic:\n",
    "\n",
    "```bash\n",
    "kafka-topics --bootstrap-server djp-training-sbox.dla-dataplatform.internal:9093   --command-config /tmp/producer_non_jaas.properties   --list\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a3fee3f",
   "metadata": {
    "tags": []
   },
   "source": [
    "\n",
    "## 4) Test Producer & Consumer\n",
    "\n",
    "Buka **2 terminal**.  \n",
    "Terminal pertama jalankan producer:\n",
    "\n",
    "```bash\n",
    "kafka-console-producer --bootstrap-server djp-training-sbox.dla-dataplatform.internal:9093   --producer.config /tmp/producer_non_jaas.properties   --topic dev.djp.test-topic.<username>\n",
    "\n",
    "\n",
    "kafka-console-producer --bootstrap-server djp-training-sbox.dla-dataplatform.internal:9093   --producer.config /tmp/producer_non_jaas.properties   --topic dev.djp.test-topic.irfan\n",
    "```\n",
    "input text dengan isi bebas.\n",
    "\n",
    "Terminal kedua jalankan consumer:\n",
    "\n",
    "```bash\n",
    "kafka-console-consumer --bootstrap-server djp-training-sbox.dla-dataplatform.internal:9093   --consumer.config /tmp/consumer_non_jaas.properties   --topic dev.djp.test-topic.irfan   --from-beginning\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a09bdd2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "\n",
    "## 5) Create Debezium Oracle CDC Connector\n",
    "\n",
    "Masuk ke **SMM â†’ Connectors â†’ Add Connector â†’ Oracle (Debezium)**.  \n",
    "Isi parameter berikut (sesuaikan `<username>`):\n",
    "\n",
    "\n",
    "### Debezium Oracle CDC Connector Config\n",
    "\n",
    "```properties\n",
    "name=oracle-cdc-<username>\n",
    "topic.prefix=trainee01\n",
    "\n",
    "# Connector class\n",
    "connector.class=io.debezium.connector.oracle.OracleConnector\n",
    "\n",
    "# Consumer overrides\n",
    "consumer.override.sasl.kerberos.service.name=kafka\n",
    "consumer.override.sasl.mechanism=GSSAPI\n",
    "consumer.override.security.protocol=SASL_SSL\n",
    "consumer.override.ssl.truststore.location=/tmp/kafka-truststore.jks\n",
    "consumer.override.ssl.truststore.password=changeit\n",
    "\n",
    "# Database connection\n",
    "database.connection.adapter=logminer\n",
    "database.dbname=XE\n",
    "database.hostname=djp-training-sbox.dla-dataplatform.internal\n",
    "database.port=1521\n",
    "database.pdb.name=XEPDB1\n",
    "database.server.name=ora1\n",
    "database.user=C##CDC\n",
    "database.password=Cdc123!\n",
    "database.include.list=\n",
    "include.schema.changes=false\n",
    "schema.include.list=CDC\n",
    "table.include.list=CDC.DEMO\n",
    "\n",
    "# Database history (consumer)\n",
    "database.history.consumer.sasl.kerberos.service.name=kafka\n",
    "database.history.consumer.sasl.mechanism=GSSAPI\n",
    "database.history.consumer.security.protocol=SASL_SSL\n",
    "database.history.consumer.ssl.truststore.location=/tmp/kafka-truststore.jks\n",
    "database.history.consumer.ssl.truststore.password=changeit\n",
    "\n",
    "# Database history (producer)\n",
    "database.history.producer.sasl.kerberos.service.name=kafka\n",
    "database.history.producer.sasl.mechanism=GSSAPI\n",
    "database.history.producer.security.protocol=SASL_SSL\n",
    "database.history.producer.ssl.truststore.location=/tmp/kafka-truststore.jks\n",
    "database.history.producer.ssl.truststore.password=changeit\n",
    "\n",
    "# Database history Kafka\n",
    "database.history.kafka.bootstrap.servers=djp-training-sbox.dla-dataplatform.internal:9093\n",
    "database.history.kafka.topic=ora1.schema.history\n",
    "\n",
    "# Producer overrides\n",
    "producer.override.sasl.kerberos.service.name=kafka\n",
    "producer.override.sasl.mechanism=GSSAPI\n",
    "producer.override.security.protocol=SASL_SSL\n",
    "producer.override.ssl.truststore.location=/tmp/kafka-truststore.jks\n",
    "producer.override.ssl.truststore.password=changeit\n",
    "\n",
    "# General\n",
    "log.mining.strategy=online_catalog\n",
    "snapshot.mode=initial\n",
    "security.protocol=SASL_SSL\n",
    "tasks.max=1\n",
    "zookeeper.connect=djp-training-sbox.dla-dataplatform.internal:2181/kafka\n",
    "secret.properties=database.password\n",
    "\n",
    "âœ… Setelah connector aktif, cek apakah data sudah mengalir ke topic:  \n",
    "`<username>.ora1.CDC.DEMO`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9909293",
   "metadata": {
    "tags": []
   },
   "source": [
    "\n",
    "## 6) Create HDFS Sink Connector\n",
    "\n",
    "Di SMM â†’ Connectors â†’ Add Connector â†’ HDFSSinkConnector. \n",
    "```properties\n",
    "{\n",
    "  \"connector.class\": \"com.cloudera.dim.kafka.connect.hdfs.HdfsSinkConnector\",\n",
    "  \"consumer.override.sasl.kerberos.service.name\": \"kafka\",\n",
    "  \"consumer.override.sasl.mechanism\": \"GSSAPI\",\n",
    "  \"consumer.override.security.protocol\": \"SASL_SSL\",\n",
    "  \"consumer.override.ssl.truststore.location\": \"/tmp/kafka-truststore.jks\",\n",
    "  \"consumer.override.ssl.truststore.password\": \"changeit\",\n",
    "  \"hadoop.conf.path\": \"file:///etc/hadoop/conf\",\n",
    "  \"hdfs.kerberos.authentication\": \"true\",\n",
    "  \"hdfs.kerberos.keytab.path\": \"${cm-agent:keytab}\",\n",
    "  \"hdfs.kerberos.namenode.principal\": \"hdfs/djp-training-sbox.dla-dataplatform.internal@DLA-TRAINING.LOCAL\",\n",
    "  \"hdfs.kerberos.user.principal\": \"${cm-agent:ENV:kafka_connect_service_principal}\",\n",
    "  \"hdfs.output\": \"/cdc/<USERNAME>\",\n",
    "  \"hdfs.uri\": \"hdfs://djp-training-sbox.dla-dataplatform.internal:8020\",\n",
    "  \"hive.metastore.uris\": \"thrift://djp-training-sbox.dla-dataplatform.internal:9083\",\n",
    "  \"key.converter\": \"org.apache.kafka.connect.storage.StringConverter\",\n",
    "  \"output.avro.passthrough.enabled\": \"true\",\n",
    "  \"output.storage\": \"com.cloudera.dim.kafka.connect.hdfs.HdfsPartitionStorage\",\n",
    "  \"output.writer\": \"com.cloudera.dim.kafka.connect.partition.writers.json.JsonPartitionWriter\",\n",
    "  \"tasks.max\": \"1\",\n",
    "  \"topics\": \"ora1.CDC.DEMO\",\n",
    "  \"value.converter\": \"org.apache.kafka.connect.json.JsonConverter\",\n",
    "  \"value.converter.passthrough.enabled\": \"true\",\n",
    "  \"value.converter.schema.registry.url\": \"https://djp-training-sbox.dla-dataplatform.internal:7790\",\n",
    "  \"value.converter.schemas.enable\": \"true\",\n",
    "  \"secret.properties\": \"\",\n",
    "  \"name\": \"HDFS_SINK-<USERNAME>\"\n",
    "}\n",
    "\n",
    "```\n",
    "Isi parameter berikut (sesuaikan `<username>`):\n",
    "\n",
    "### HDFS Sink Connector Config\n",
    "\n",
    "```properties\n",
    "name=hdfs-sink-<username>\n",
    "connector.class=com.cloudera.dim.kafka.connect.hdfs.HdfsSinkConnector\n",
    "tasks.max=1\n",
    "topics=<username>.ora1.CDC.DEMO\n",
    "\n",
    "# HDFS target\n",
    "hdfs.uri=hdfs://djp-training-sbox.dla-dataplatform.internal:8020\n",
    "hdfs.output=/cdc/<username>/\n",
    "hadoop.conf.path=/etc/hadoop/conf\n",
    "hdfs.kerberos.authentication=true\n",
    "\n",
    "# Converter\n",
    "value.converter=org.apache.kafka.connect.json.JsonConverter\n",
    "value.converter.schemas.enable=true\n",
    "value.converter.schema.registry.url=https://djp-training-sbox.dla-dataplatform.internal:7790\n",
    "\n",
    "# Hive Metastore\n",
    "hive.metastore.uris=thrift://djp-training-sbox.dla-dataplatform.internal:9083\n",
    "\n",
    "Cek hasil di HDFS:\n",
    "\n",
    "```bash\n",
    "hdfs dfs -ls /cdc/<username>/\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7aa2125",
   "metadata": {},
   "source": [
    "\n",
    "## 7) (Optional) Spark Structured Streaming\n",
    "\n",
    "Ubah File kafka_spark_consumer.py pada bagian `<username>`:\n",
    "```properties\n",
    "#CONFIG SPARK CONSUMER\n",
    "KAFKA_BS  = \"djp-training-sbox.dla-dataplatform.internal:9093\"\n",
    "TRUST_JKS = \"/tmp/kafka-truststore.jks\"\n",
    "TOPIC     = \"<username>.ora1.CDC.DEMO\"\n",
    "OUT_DIR   = \"hdfs://djp-training-sbox.dla-dataplatform.internal:8020/cdc/spark-<username>/\"\n",
    "CHK_DIR   = \"hdfs://djp-training-sbox.dla-dataplatform.internal:8020/checkpoints/cdc_kafka_to_hdfs-<username>\"\n",
    "CONSUMER_GROUP = \"spark-consumer-<username>\"  \n",
    "```\n",
    "SAVE FILE (ctrl + S) !\n",
    "\n",
    "Jika ingin membaca dari Kafka dan tulis ke HDFS dengan Spark:\n",
    "Ubah  pada bagian `<username>`\n",
    "```bash\n",
    "spark3-submit   --master local[*]   --conf spark.driver.extraJavaOptions=\"-Djava.security.auth.login.config=/etc/kafka/kafka_client_jaas.conf\"   --conf spark.executor.extraJavaOptions=\"-Djava.security.auth.login.config=/etc/kafka/kafka_client_jaas.conf\"   /home/trainee01/labs/01_oracle_debezium_kafka_hdfs/kafka_spark_consumer.py\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00b7b26b",
   "metadata": {},
   "source": [
    "\n",
    "## 8) Validation Checklist\n",
    "\n",
    "- âœ… CDC Connector di SMM status **RUNNING**.  \n",
    "- âœ… Topic `dev.djp.ora1.CDC.DEMO` berisi data perubahan dari Oracle.  \n",
    "- âœ… HDFS Sink menghasilkan file di `/cdc/<username>/`.  \n",
    "- âœ… (Optional) Spark job menulis file JSON di HDFS dengan checkpoint.  \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
